{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9454b5e9",
   "metadata": {},
   "source": [
    "<em style=\"text-align:center\">Copyright Iván Pinar Domínguez</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2128d1c-62d1-4681-abe1-754a1410e632",
   "metadata": {},
   "source": [
    "## 0.Importar librerías iniciales e instancia de modelo de chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da270ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate, SystemMessagePromptTemplate,ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain.agents import load_tools,initialize_agent,AgentType,create_react_agent,AgentExecutor\n",
    "f = open('../OpenAI_key.txt')\n",
    "api_key = f.read()\n",
    "llm = ChatOpenAI(openai_api_key=api_key,temperature=0) #Recomendable temperatura a 0 para que el LLM no sea muy creativo, vamos a tener muchas herramientas a nuestra disposición y queremos que sea más determinista"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6a887",
   "metadata": {},
   "source": [
    "## 1.Cargamos la BD Vectores y el compresor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0cd8686",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Podríamos establecer que tuviera memoria\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\") #ponemos una denominada clave a la memoria \"chat_history\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1c2fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import SKLearnVectorStore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "funcion_embedding = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "persist_path=\"../Bloque 3_Conectores de Datos/ejemplosk_embedding_db\"\n",
    "vector_store_connection = SKLearnVectorStore(embedding=funcion_embedding, persist_path=persist_path, serializer=\"parquet\")\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(base_compressor=compressor, base_retriever=vector_store_connection.as_retriever())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f0f7b",
   "metadata": {},
   "source": [
    "## 2.Creamos una nueva herramienta a partir de la BD Vectorial para obtener resultados optimizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a390f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dbe9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def consulta_interna(text: str) -> str:\n",
    "    '''Retorna respuestas sobre la historia de España. Se espera que la entrada sea una cadena de texto\n",
    "    y retorna una cadena con el resultado más relevante. Si la respuesta con esta herramienta es relevante,\n",
    "    no debes usar ninguna herramienta más ni tu propio conocimiento como LLM'''\n",
    "    compressed_docs = compression_retriever.invoke(text)\n",
    "    resultado = compressed_docs[0].page_content\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b221e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = load_tools([\"wikipedia\",\"llm-math\"],llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6443291",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools=tools+[consulta_interna]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430a1e2b",
   "metadata": {},
   "source": [
    "## 3.Creamos el agente y lo ejecutamos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de033456",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(tools, llm, agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,memory=memory,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356c45df",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\"¿Qué periodo abarca cronológicamente en España el siglo de oro?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bea07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\"¿Qué pasó durante la misma etapa en Francia?\") #Gracias a tener memoria compara en esas fechas qué ocurrió en Francia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59281e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.invoke(\"¿Cuáles son las marcas de vehículos más famosas hoy en día?\") #Pregunta que no podemos responder con nuestra BD Vectorial"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
